# ---------------------------
# Ollama Modelfile for loading a local GGUF model
# ---------------------------
# This file tells Ollama how to load and use a quantized model (GGUF format).
# You can create an Ollama model from this file using:
#   ollama create my-llama3-local -f Modelfile
# Then run it with:
#   ollama run my-llama3-local
# ---------------------------

# Load the local GGUF model from the ./models directory
FROM ./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf

# ---------------------------
# Parameters (sampling / generation settings)
# ---------------------------
# temperature: randomness (lower = more deterministic, higher = more creative)
# top_p: nucleus sampling â€” keeps tokens with cumulative probability below top_p
# top_k: limits the candidate tokens to the top_k most likely choices
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40

# ---------------------------
# Chat template (defines how prompts are wrapped)
# ---------------------------
# This sets the format Ollama will use to talk with the model.
# You can customize this to follow specific chat formatting.
TEMPLATE """
{{ .System }}

User: {{ .Prompt }}
Assistant:
"""
